---
title: "Main"
author: "Vikas Arun(va2298), Xuehan Liu(xl2615), Sean Reddy(sr3336), Boxuan Zhao(bz2290)"
date: "4/26/2017"
output: pdf_document
---

###Introduction

In this project, we look at auction items' price on sothebys.com and focus on trying to predict the sale price of paintings auctioned on the website. Inspiration for the project comes for the inaccuracy of the predictions presented on Sotheby's site. In order to predict the price we perform text mining on all the text describing the painting, as well as image analysis on the image of the painting itself. We first scrape information, including descriptions and images of each items, and then do cleaning-up and image feature extraction. After obtaining all the features, we fit a model and fit it to our dataset with features.  

###Step 0: Install and Import Relevant Packages

Changed the working directory to the /doc folder on your local machine.
```{r}
#Package needed
library(glmnet)
install.packages("glmnet")
intall.packages("Hmisc")
library("Hmisc")
setwd("/Users/xuehan/Desktop/Spr2017-proj5-grp8/doc") #change to your local path
```


###Step 1: Pull data from sothebys.com

The Sothebys web scraping program was written in Python and can be accessed through "GoldDust_1.0.ipynb", and viewing the file through a Jupyter Notebook. The information was scraped by going to each auction page, downloading the item names, and then navigating to each item page to download the title, image, description, sale price, predicted price, and many other features. These items were outputted to two places. The text data was outputted to "auctionItems.csv" in the /data folder and the images were outputted to "Images_full", also in the /data folder. We scraped ~3000 painting items from Sothebys, however Sotheby's classification of auctions isn't perfect, so the data didn't contain solely paintings.


###Step 2: Feature clean up and extractions 
We scraped information from Sotheby's website and the initial features we have from the csv file are the following:

"auctionDate","auctionId","auctionName","currency","genre", "guaranteeLine","highEst","id","isSold","lowEst","medium","salePrice","title","type", "auctionYear", "lot_desc"

We created categorical variables(1s and 0s) History, Portrait, Landscape, Genre, StillLife, Abstract, and Other from the column "type", in which we utilized regrular expressions to extract type for each paintings.

In addition, we also created diff variable, which is estimated by the increased proportion from highEst and lowEst.

More categorical variables, famous and sold, which record whether each painting is owned by famous people and whether it is sold, based on the information we extract from lot_desc and isSold.

At last, we extract the height and width of the paintings from the lot_desc and form the final two features from our data set.

In addition to the csv file, we have also created HOG feature as well as RGB feature from the paintings. 

####Step 2a: Data clean-up and feature extraction
```{r}
source("../lib/FeatureConstruction.R")
my.dat = read.csv("../data/auctionItems.csv",header = TRUE, stringsAsFactors = FALSE)
feature.csv = Feature.Construction(my.dat)#Construct the first type of features
#dim(feature.csv)
animal<-read.csv("../output/AnimalFeature.csv")
colnames(animal)<-c("ID","animal")

#Rename ID for each observations in the dataset
img_dir_full <- "/Users/xuehan/Desktop/Images/" #local path of 3000 images
file.names <- list.files(img_dir_full,pattern="*.jpg") 
file.names.short<-NULL
for (i in 1:length(file.names)){
  file.names.short[i]<-substr(file.names[[i]],1,nchar(file.names[[i]])-4)
}

for (i in 1:nrow(animal)){
  animal$ID[i]<-file.names.short[i]
}

#Merge two datasets
data.full1<-merge(feature.csv,animal,by.x="ID",by.y="ID")
```

####Step 2b: Images feature extraction 
In this step, I extracted HoG feature, RGB feature and VGG feature from the images that associate with each auction item. Since most of our observations are paintings, it makes sense that we extract information from those paintings directly. 

Notice that the images we used are too large too upload on Gitgub. Alternatively, all 3000 images are in the Google drive (https://drive.google.com/drive/folders/0BzExzKbmVUJ2R3dsYUVYUXNvMVk?usp=sharing). Images are stored seperated in three different files (image_2500,image_5000, and image_7000). If you are interested in reproduce this project, please download them and move them all in the same folder for future use. Anyone with a LionMail should be able to access the google drive folder. 

```{r}
#After downloading the images to your local path, please go to the "image_feature_extraction.R" and change "img_dir_full" to your local path (located after loading libraries).
#Then run the following code
source(../lib/image_feature_extraction.R)

#If the above function took too long to run (3000 images), you can manually run rgb_seperate.R under doc folder. It generates the whole 3000 by 512 matrix in four chuncks, meaning from the 1th to 500th, 501th to 1000th and so on.


###HoG Feature

HoG<-read.csv("../data/HoG.csv",header=T)[,-1]
head(HoG)


###RGB Feature
RGB<-read.csv("../data/RGB.csv",header=T)[,-1]
head(RGB)


###VGG Feature


```

####Step 2c: Merge all sub-datasets that contain features together to generate a functional dataset with full features for model fitting.

```{r}
#Merge datasets by paintings' ID which is the last column
head(HoG[,ncol(HoG)])
colnames(HoG)[ncol(HoG)]<-"ID"
head(RGB[,ncol(RGB)])
colnames(RGB)[ncol(RGB)]<-"ID"

data.full<-merge(HoG,RGB,by.x="ID",by.y="ID")
colnames(data.full)[2:55]<-paste("HoG",1:54,sep="")
colnames(data.full)[56:567]<-paste("RGB",1:512,sep="")
#store full data
data_full<-merge(data.full,data.full1,by.x="ID",by.y="ID" )
write.csv(data_full,file="../data/data_full.csv")


```


###Step 3: Fit the model and find those important features in order to predict auction price

For this part, we first read in and clean the data, then we apply the regular regression method, which result in a non-satifiactory result. Therefore, we then start to implement penalized regression methods and achieve a much better results.
```{r}
#read in the cleaned up data
data_full<-read.csv("../data/data_full.csv")
data<-read.csv("../data/auctionItems.csv")[,c(3,9,13)]
data$Id<-paste0(data$auctionId,"_",data$id)
data<-data[,-c(1,2)]
data_full<-merge(data_full,data,by.x="ID",by.y="Id")
colnames(data_full)[3:56]<-paste("HoG",1:54,sep="")
colnames(data_full)[57:568]<-paste("RGB",1:512,sep="")

####Regular regression methods:
#We randomly split 25% of our data as testing data set:
ind<-sample(1:dim(data_full)[1],dim(data_full)[1] * 0.25)
test<-data_full[ind,]
train<-data_full[-ind,]
score<- lm(salePrice~.,train[,-c(1,2)])#regular regression
regular.prediction = predict(score,test[,-c(1,2)])#Store predictions from regular regression model
na.position = which(is.na(regular.prediction))#Locate na positions
regular.prediction = regular.prediction[-na.position]#Remove all na predicitons from regular regression predictions

#Produce statistics for comparison
regular.summary = summary(predict(score,test[,-c(1,2)])-test$salePrice[-na.position])
regular.mse = sum((regular.prediction-test$salePrice[-na.position])^2)

####Advanced regression methods : LASSO Regression
#We randomly split 25% of our data as testing data set:
data_omit = na.omit(data_full)#Remove any rows that has na valuse in it
data_omit = data_omit[,-c(1,2)]#Remove non-useful columns
ind<-sample(1:dim(data_omit)[1],dim(data_omit)[1]*0.25)
test<-data_omit[ind,]
train<-data_omit[-ind,]
lambda = cv.glmnet(as.matrix(train[,-c(580)]),as.matrix(train[,c(580)]),type.measure = "mse")$lambda.min#Cross validate to find the best lambda values
score_lasso = glmnet(as.matrix(train[,-c(580)]),as.matrix(train[,c(580)]),family="gaussian",alpha=1,lambda = lambda)#Fit lasso model
lasso.prediction = predict(score_lasso,as.matrix(test[,-c(580)]))#Prodce prediction

#Produce statistics for comparison
lasso.summary = summary(prediction - as.numeric(test[,580]))
lasso.mse = mean((lasso.prediction-test[,-c(580)])^2)
```

###Conclusion

So, we finally compare the results from two regression methods
```{r}
#Mse
regular.mse

lasso.mse

#Summary Statistics
regular.summary

lasso.summary
```

